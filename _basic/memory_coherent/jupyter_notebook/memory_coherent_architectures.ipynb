{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96f12d08-1a3a-49b6-8371-596a52cfad09",
   "metadata": {},
   "source": [
    "Before we begin, let us execute the below cell to display information about the NVIDIA® CUDA® driver and the GPUs running on the server by running the `nvidia-smi` command. To do this, execute the cell block below by clicking on it with your mouse, and pressing Ctrl+Enter, or pressing the play button in the toolbar above. You should see some output returned below the grey cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05de4237-f29a-4958-8a6c-81c9668d0e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e82a96-f59d-438b-aaa0-a7ec58766f76",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "The **goal** of this lab is to:\n",
    "- Learn how Memory Coherent Architectures like Grace Hopper Superchip help achieve better productivity.\n",
    "- Understand how applications gain incremental acceleration using chip-to-chip connectivity between CPU and GPU\n",
    "- We do not intend to cover:\n",
    "    - Optimization techniques like memory prefetching API and cache eviction policies, which could optimize the performance further for applications.\n",
    "\n",
    "Let us start by revising some fundamentals covered previously.\n",
    "\n",
    "**NOTE: This notebook is to run on a Grace Hopper Superchip only.**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b78c8f-6a42-4d92-a648-3d573794487c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Heterogeneous Computing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a50a72f-5a79-4278-8add-dc680a9cfe86",
   "metadata": {},
   "source": [
    "Heterogenous Computing can be defined as combining processors of different types, each specializing in different types of execution.\n",
    "\n",
    "A Heterogeneous programming model like CUDA includes provisions for a CPU and GPU. These enable developers to target portions of source code for parallel execution on the device (GPU). The programming model provides functions that can be executed on the host (CPU) to interact with the device. The two processors that work with each other are:\n",
    "\n",
    "- Host: CPU and its memory (Host Memory)\n",
    "- Device: GPU and its memory  (Device Memory)\n",
    "\n",
    "  \n",
    "<img src=\"../../_common/images/heterogeneous_computing.jpg\" width=\"80%\" height=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c000fa-c479-4021-820a-e5cdb8446cbe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Steps in Heterogenous Computing Programming Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6afc31bc-5726-4895-bc73-5d60636b19b2",
   "metadata": {},
   "source": [
    "The table below highlights the typical steps that have been required so far to convert sequential codes to target Heterogenous computing architectures using CUDA and OpenACC as an example :\n",
    "\n",
    "| Sequential code | CUDA Code | OpenACC Code |\n",
    "| --- | --- | --- |\n",
    "| **Step 1** Allocate memory on the CPU ( _malloc new_ ) | **Step 1** : Allocate memory on the CPU (_malloc, new_ )| Allocate memory on the CPU (_malloc, new_ ) |\n",
    "| **Step 2** Populate/initialize the CPU data | **Step 2** Allocate memory on the GPU, using API like _cudaMalloc()_ | **Step 2** Allocate memory on the GPU, using pragma like _#pragma acc data create_ |\n",
    "| **Step 3** Call the CPU function that has the crunching of data. | **Step 3**  Populate/initialize the CPU  |  **Step 3**  Populate/initialize the CPU  |\n",
    "| **Step 4** Consume the crunched data on Host | **Step 4** Transfer the data from the host to the device with _cudaMemcpy()_ |  **Step 4** Transfer the data from the host to the device with _#pragma acc data copy_  |\n",
    "| | **Step 5** Call the GPU function with _<<<,>>>_ brackets | **Step 5** Parallelize and call the GPU function with _#pragma acc parallel loop_ |\n",
    "| | **Step 6** Synchronize the device and host with _cudaDeviceSynchronize()_ | **Step 6** Synchronize the device and host with _#pragma acc wait_ |\n",
    "| | **Step 7** Transfer data from the device to the host with _cudaMemcpy()_ | **Step 7** Transfer data from the device to the host with _#pragma acc data copy_ |\n",
    "| | **Step 8** Consume the crunched data on Host | **Step 8** Consume the crunched data on Host |\n",
    "\n",
    "CPU and GPU memory are different, and the developer needs to use additional  API to allocate and free memory on GPU. This traditionally created an inertia for developers to adopt heterogeneous computing due to a complex programming model that involves manually managing device memory allocations and data transfer to and from the host.\n",
    "    \n",
    "### Unified Memory\n",
    "With every new CUDA and GPU architecture release, new features are added. These new features provide more performance and ease of programming or allow developers to implement new algorithms that otherwise weren't possible to port on GPUs using CUDA.\n",
    "One such important feature that was released from CUDA 6.0 onward and finds its implementation from the Kepler GPU architecture is unified memory (UM). \n",
    "\n",
    "In simpler words, UM provides the user with a view of a single memory space that's accessible by all GPUs and CPUs in the system. This is illustrated in the following diagram:\n",
    "\n",
    "<img src=\"../../_common/images/UM.png\" width=\"60%\" height=\"60%\">\n",
    "\n",
    "\n",
    "UM simplifies programming efforts for beginners as developers need not explicitly manage to copy data to and from GPU. Below is an example usage of how to use managed memory using different programming models covered previously:\n",
    "\n",
    "<details>\n",
    "    <summary markdown=\"span\"><b>CUDA C/C++</b></summary>\n",
    "\n",
    "```cpp\n",
    " // Allocate Unified Memory -- accessible from CPU or GPU\n",
    "  int *a, *b, *c;\n",
    "  cudaMallocManaged(&a, N*sizeof(int));\n",
    "  cudaMallocManaged(&b, N*sizeof(int));\n",
    "  cudaMallocManaged(&c, N*sizeof(int));\n",
    "  ...\n",
    "\n",
    "  // Free memory\n",
    "  cudaFree(a);\n",
    "  cudaFree(b);\n",
    "  cudaFree(c);\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "    <summary markdown=\"span\"><b>CUDA Fortran</b></summary>\n",
    "   \n",
    "```fortran\n",
    "!matrix data\n",
    "real, managed, allocatable, dimension(:,:) :: A, B, C\n",
    "```\n",
    "</details>\n",
    "\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "    <summary markdown=\"span\"><b>OpenACC</b></summary>\n",
    "    \n",
    "In OpenACC we used the Unified Memory feature by enabling the compilation time flag **-acc=gpu -gpu=managed**. OpenACC parallel and loop directives relied on a feature called [CUDA Managed Memory](../../_common/jupyter_notebook/GPU_Architecture_Terminologies.ipynb) to deal with the separate CPU & GPU memories for us. Just adding OpenACC to our loop, we achieved a considerable performance boost without using explicit data copy directives. \n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "    <summary markdown=\"span\"><b>Standard Languages</b></summary>\n",
    "    \n",
    "C++ stdpar and DO CONCURRENT in Fortran provide the highest portability and can be seen as the first step to porting on Heterogeneous computing architectures like GPU. The implementation depends on features like Unified memory to make sure the standard languages support varied heterogeneous computing architectures without any vendor-specific functionality.  \n",
    "\n",
    "</details>\n",
    "\n",
    "### Memory Coherent Architectures and Unified Memory\n",
    "\n",
    "Memory coherency is a feature where the multiple processes/threads accessing memory must agree on the state of the memory at all times. For example, thread 0 reads the memory location X, and thread 1 reads the same location at the same time, both threads should always read the same value. But if memory is not coherent, threads A and B might read back different values.\n",
    "\n",
    "Memory coherency can be provided natively at the hardware level or emulated at the software level. For example, old GPU architecture from NVIDIA supported memory coherency via Unified Memory using a Heterogeneous Memory Management Module, also referred to as [HMM](https://on-demand.gputechconf.com/gtc/2017/presentation/s7764_john-hubbardgpus-using-hmm-blur-the-lines-between-cpu-and-gpu.pdf) feature that use software to emulate memory coherence between CPUs and GPUs. Software-based memory coherency provides its own limitation and is not required in latest generation architectures like Grace Hopper Superchip. The applications transparently benefit from hardware acceleration for memory coherency provided by NVLink-C2C, without any software changes, as described in the next section.\n",
    "\n",
    "Two main ways to obtain Unified Memory are as follows:\n",
    "\n",
    "- Fully Supported Unified Memory: Developers allocate memory on the host with system APIs: stack variables, global-/file-scope variables, malloc() / mmap() thread locals, etc.\n",
    "- Managed Unified Memory: Memory allocated using explicit API, for example, cudaMallocManaged(). This provides backward compatibility and is available on more systems, and may perform better than System-Allocated Memory.\n",
    "\n",
    "CUDA API provides functions to query the device to check the support for Unified Memory. The following example shows how to detect the Unified Memory support level at runtime: \n",
    "\n",
    "```\n",
    "//Support Full Unified memory like in Grace Hopper SuperChip\n",
    "cudaDeviceGetAttribute(&pma, cudaDevAttrPageableMemoryAccess, d);\n",
    "//Support Managed Unified Memory in older architectures \n",
    "cudaDeviceGetAttribute(&cma, cudaDevAttrConcurrentManagedAccess, d);\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f91b21-cb15-48ca-ac4e-662be3313976",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../source_code && nvcc -o unified_test unified_test.cu && ./unified_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36578ac-7a13-494a-aa4e-91f330ca9f43",
   "metadata": {},
   "source": [
    "Let us now delve deeper into the NVIDIA Grace Hopper Superchip architecture to understand the support of Memory Coherency. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d32f1d-b622-4f1f-915a-fe97d365c25c",
   "metadata": {},
   "source": [
    "## Grace Hopper Superchip "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b004c11-4437-4674-a9bd-2420a6256e13",
   "metadata": {},
   "source": [
    "The NVIDIA Grace Hopper Superchip architecture was announced in 2023 and is one of the latest architectures in production by NVIDIA. The Superchip brings together the high throughput-based performance of the [NVIDIA Hopper GPU](https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/) with the versatility of the [NVIDIA Grace CPU](https://www.nvidia.com/en-us/data-center/grace-cpu/). The two processors are connected to each other by high bandwidth and memory coherent [NVIDIA NVLink Chip-2-Chip (C2C) interconnect](https://www.nvidia.com/en-us/data-center/nvlink-c2c/) in a single superchip.\n",
    "\n",
    "<img src=\"../../_common/images/grace_hopper_arch.jpg\" width=\"80%\" height=\"80%\">\n",
    "\n",
    "While there is much more to the Grace Hopper superchip, this lab focuses on the memory coherency aspect using NVIDIA NVLink-C2C, which is an NVIDIA memory coherent, high-bandwidth, and low-latency superchip interconnect. It is the heart of the Grace Hopper Superchip and delivers up to 900 GB/s total bandwidth. This is 7x higher bandwidth than x16 PCIe Gen5 lanes commonly used in accelerated systems. \n",
    "\n",
    "Memory coherency enables developers to transfer only the data needed, and not migrate entire pages to and from the GPU.  NVIDIA NVLink-C2C hardware coherency enables the Grace CPU to cache GPU memory at cache-line granularity and for the GPU and CPU to access each other’s memory without page migrations.\n",
    "\n",
    "Let us run the Linux Operating System(OS) command to verify that GPU memory is indeed visible to the OS. By running the `numactl` command, ideally you should see the output as follows: \n",
    "\n",
    "<img src=\"../../_common/images/grace_hopper_numactl.jpg\" width=\"80%\" height=\"80%\">\n",
    "\n",
    "The output shows that there are two NUMA nodes on this machine,  and how much memory is available for each node. Please note  the last seven NUMA nodes can be ignored if [MIG](https://www.nvidia.com/en-in/technologies/multi-instance-gpu/) is not used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c82056a-54a8-4e02-96d1-8b4bb89fc566",
   "metadata": {},
   "outputs": [],
   "source": [
    "!numactl -H"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c33096-eb03-419d-b527-25e95ed77614",
   "metadata": {},
   "source": [
    "Let us also use a tool provided by NVIDIA for bandwidth measurements on NVIDIA GPUs. The test measures bandwidth for various memcpy patterns across different links. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d14f70b-26a9-40ef-9370-ed4a739ae2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cd /nvbandwidth && ./nvbandwidth -t host_to_device_bidirectional_memcpy_ce\n",
    "!cp -r -n /nvbandwidth ../source_code/ && cd ../source_code/nvbandwidth && ./nvbandwidth -t host_to_device_bidirectional_memcpy_ce "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb239841-d8e3-463d-ad05-54ff6b491846",
   "metadata": {},
   "source": [
    "For NVIDIA Grace Hopper Superchip ideally we should get the following bi-directional bandwidth between CPU and GPU.\n",
    "\n",
    "```\n",
    "Running host_to_device_bidirectional_memcpy_ce.\n",
    "memcpy CE CPU(row) <-> GPU(column) bandwidth (GB/s)\n",
    "          0\n",
    "0    240.64\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7e0a402-fe41-4e6f-aab8-4528bf9bf437",
   "metadata": {},
   "source": [
    "Let us understand in more detail the aspect of access to memory by Grace CPU and Hopper GPU in the next section. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb1facf-5f0d-4623-b226-167f1560ac6e",
   "metadata": {},
   "source": [
    "### Global Access\n",
    "\n",
    "Grace can read Hopper’s memory and Hopper can read Grace’s memory directly, without moving any data. It’s possible via the NVLink C2C connection. Not only that, but the CPU cache can cache both its memory and the GPU memory, by keeping track of GPU cache lines and CPU cache lines. This means memory accesses from either processor are coherent and the total memory is truly unified.\n",
    "\n",
    "<img src=\"../../_common/images/grace_hopper_global_access.jpg\" width=\"75%\" height=\"75%\">\n",
    "\n",
    "- Grace directly reading Hopper’s memory: CPU fetches GPU data into CPU L3 cache. The cache remains coherent with GPU memory. Changes to GPU memory evict cache line.\n",
    "- Hopper directly reading Grace’s memory: GPU loads CPU data via CPU L3 cache. CPU and GPU can both hit on cached data. Changes to CPU memory update cache line.\n",
    "\n",
    "Address Translation Service (ATS), as shown in the figure below, enables the CPU and GPU to share a single per-process page table, enabling all CPU and GPU threads to access all system-allocated memory, which can reside on physical CPU or GPU memory. ATS enables the CPU and GPU to share a single per-process page table, enabling all CPU and GPU threads to access all system-allocated memory, which can reside on physical CPU or GPU memory.\n",
    "\n",
    "<img src=\"../../_common/images/grace_hopper_ATS.jpg\" width=\"75%\" height=\"75%\">\n",
    "\n",
    "### Memory Allocators Impact Data Placement and Movement\n",
    "\n",
    "Based on the type of memory allocator and heterogeneous computing architecture being used can impact the data placement and movement. This behaviour can be summarized as follows: \n",
    "\n",
    "|  | cudaMalloc | cudaMallocManaged | System API (malloc/mmap/...) |\n",
    "| --- | --- | --- | --- |\n",
    "| Placement | GPU | First Touch | First Touch |\n",
    "| Which Processor Can Access? | GPU | Both CPU and GPU | Both CPU and GPU |\n",
    "| How does access happen? | GPU MMU | Fault on first access and move page | Direct access over C2C using Automatic Translation Service  |\n",
    "| Any optimization by Driver? | None | Fault or access counter  | Using access counter to migrate memory between CPU and GPU |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afde085e-1225-455e-8b03-53b588a85b99",
   "metadata": {},
   "source": [
    "## Application Categories"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b84f9f7f-0fe6-4e3a-86d5-3f79dc00aea5",
   "metadata": {},
   "source": [
    "An analysis is required to estimate the speedup gained while porting and deploying the applications from previous non-memory coherent architectures to memory coherent architectures like Grace Hopper Superchip. Broadly, they can be categorized into the following: \n",
    "\n",
    "- **CPU Only Application**:  Applications that work well on latency-reducing architectures like CPU or are yet to be migrated to GPU.\n",
    "- **Fully GPU Accelerated**: Application which has been majorly ported to GPU and computation happens almost fully on the GPU with data in GPU memory\n",
    "- **Partially GPU Accelerated**: Application which partially computes on CPU and GPU. They can be further subdivided into two categories\n",
    "    - **Bound by CPU**: As GPU becomes faster, the majority of the time of application is spent in CPU computation\n",
    "    - **Bound by CPU<->GPU communication**: Due to back-and-forth data transfers between two different memories, the application hotspot is the transfer and is bottlenecked by the transfer speed.\n",
    "\n",
    "\n",
    "\n",
    "| | Performance Improvement | Productivity Improvements |\n",
    "| --- | --- | --- |\n",
    "| Fully GPU Accelerated | These applications primarily will benefit from the increase in computing power of GPU and minimal performance gain can be seen by using C2C. | Memory-coherent programming approach can improve developer productivity by reducing the learning curve |\n",
    "| CPU bound Application | These applications primarily will benefit from the increase in computing power of GRACE CPU | Memory-coherent programming approach can improve developer productivity by reducing the learning curve | \n",
    "| Bound by CPU<->GPU communication|  These applications will primarily benefit from C2C interconnect, reducing the time to transfer data/pages and using features like ATS | Memory-coherent programming approach can improve developer productivity by reducing the learning curve |\n",
    "\n",
    "The table above does not consider any aspect of porting code to the latest versatile and energy-efficient ARM-based NVIDIA Grace CPU. We will cover the same separately at the end of this lab.\n",
    "\n",
    "Let us now understand how to port the application using system API like `malloc` directly and analyze the same using profilers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60afb979-b834-49ff-b9b1-3ce689bc68ce",
   "metadata": {},
   "source": [
    "## The Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855fdc8f-1d28-4c6f-83a7-e662c43e5f56",
   "metadata": {},
   "source": [
    "This section provides an overview of the scientific problem we focus on and the solver we employ. Then, we execute the single GPU version of the application program.\n",
    "\n",
    "### Laplace Equation\n",
    "\n",
    "Laplace Equation is a well-studied linear partial differential equation that governs steady-state heat conduction, irrotational fluid flow, and many other phenomena. \n",
    "\n",
    "In this lab, we will consider the 2D Laplace Equation on a rectangle with [Dirichlet boundary conditions](https://en.wikipedia.org/wiki/Dirichlet_boundary_condition) on the left and right boundary and periodic boundary conditions on the top and bottom boundary. We wish to solve the following equation:\n",
    "\n",
    "$\\Delta u(x,y) = 0\\;\\forall\\;(x,y)\\in\\Omega,\\delta\\Omega$\n",
    "\n",
    "### Jacobi Method\n",
    "\n",
    "The Jacobi method is an iterative algorithm to solve a linear system of strictly diagonally dominant equations. The governing Laplace equation is discretized and converted to a matrix amenable to Jacobi-method-based solver. The pseudo-code for the Jacobi iterative process can be seen in the diagram below:\n",
    "\n",
    "<img src=\"../../_common/images/jacobi_algo.jpg\" width=\"90%\" height=\"90%\">\n",
    "\n",
    "\n",
    "The outer loop defines the convergence point, which could either be defined as reaching the max number of iterations or when [L2 Norm](https://link.springer.com/referenceworkentry/10.1007%2F978-0-387-73003-5_1070) reaches a max/min value. \n",
    "\n",
    "\n",
    "### The Code\n",
    "\n",
    "The GPU processing flow, in general, follows three key steps:\n",
    "\n",
    "1. Copy data from CPU to GPU\n",
    "2. Launch GPU Kernel\n",
    "3. Copy processed data back to the CPU from the GPU\n",
    "\n",
    "<img src=\"../../_common/images/gpu_programming_process.png\" width=\"70%\" height=\"70%\">\n",
    "\n",
    "We follow the same three steps in our code. Let's understand the single GPU code first. \n",
    "\n",
    "The source code is available at [jacobi.cu](../source_code/jacobi.cu) (click to open). Similarly, have a look at the [Makefile](../source_code/Makefile). \n",
    "\n",
    "Refer to the `single_gpu(...)` function. The important steps at each iteration of the Jacobi Solver inside `while` loop are:\n",
    "1. The norm is set to 0 using `cudaMemset`.\n",
    "2. The device kernel `jacobi_kernel` is called to update the interior points.\n",
    "3. The norm is copied back to the host using `cudaMemcpy` (DtoH), and\n",
    "4. The periodic boundary conditions are re-applied for the next iteration using `cudaMemcpy` (DtoD).\n",
    "\n",
    "```\n",
    "    while (l2_norm > tol && iter < iter_max) {\n",
    "        cudaMemset(l2_norm_d, 0, sizeof(float));\n",
    "\n",
    "\t   // Compute grid points for this iteration\n",
    "        jacobi_kernel<<<dim_grid, dim_block>>>(a_new, a, l2_norm_d, iy_start, iy_end, nx);\n",
    "       \n",
    "        cudaMemcpy(l2_norm_h, l2_norm_d, sizeof(float), cudaMemcpyDeviceToHost);\n",
    "\n",
    "        // Apply periodic boundary conditions\n",
    "        cudaMemcpy(a_new, a_new + (iy_end - 1) * nx, nx * sizeof(float), cudaMemcpyDeviceToDevice);\n",
    "        cudaMemcpy(a_new + iy_end * nx, a_new + iy_start * nx, nx * sizeof(float),cudaMemcpyDeviceToDevice);\n",
    "\n",
    "\t    cudaDeviceSynchronize();\n",
    "\t    l2_norm = *l2_norm_h;\n",
    "\t    l2_norm = std::sqrt(l2_norm);\n",
    "\n",
    "        iter++;\n",
    "\t    if ((iter % 100) == 0) printf(\"%5d, %0.6f\\n\", iter, l2_norm);\n",
    "        std::swap(a_new, a);\n",
    "    }\n",
    "```\n",
    "\n",
    "Note that we run the Jacobi solver for 1000 iterations over the grid. The code is present and can be studied here: [C/C++ version](../source_code/jacobi.cu) and [Makefile](../source_code/Makefile). Let's compile the code by running the below cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be373bca-2b83-4f79-863c-d6c73af0ebe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../source_code && make clean && make && make run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046101f8-6bbe-43b6-bb5c-835e491170ec",
   "metadata": {},
   "source": [
    "The output reports the norm value every 100 iterations and the total execution time of the Jacobi Solver. The expected output is:\n",
    "\n",
    "```\n",
    "Single GPU jacobi relaxation: 1000 iterations on 16384 x 16384 mesh\n",
    "    0, 31.999022\n",
    "  100, 0.897983\n",
    "  200, 0.535684\n",
    "  300, 0.395651\n",
    "  400, 0.319039\n",
    "  500, 0.269961\n",
    "  600, 0.235509\n",
    "  700, 0.209829\n",
    "  800, 0.189854\n",
    "  900, 0.173818\n",
    "16384x16384: 1 GPU:   4.4512 s\n",
    "```\n",
    "\n",
    "The execution time may differ depending on the GPU, but the norm value after every 100 iterations should be the same. The program accepts `-nx` and `-ny` flags to change the grid size (preferably a power of 2) and `-niter` flag to change the number of iterations.\n",
    "\n",
    "### Profiling analysis\n",
    "\n",
    "Before we profile the code, we would need to configure the Nsight Systems to capture C2C events. To achieve this, we use the following option:\n",
    "\n",
    "- `--event-sample=system-wide`, collects system-wide event samples.\n",
    "- `--cpu-socket-events='comma separated events'`, collects per-socket Uncore PMU counters.\n",
    "\n",
    "We choose a few events to show addional information with respect to C2C events. For more details on the events available, run the following cells: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bd7b84-0cbe-453b-af43-da01570ca765",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nsys profile --cpu-socket-events=help:All"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b5bd30-b6d0-4ad3-9bcf-86676b0c8d5b",
   "metadata": {},
   "source": [
    "Below is an example output for the chosen events.\n",
    "\n",
    "```\n",
    "'61'\tSocket_0_C2C0/rd_bytes_local\n",
    "\tIn a system with the GPU connected to the SoC via the C2C bus, counts the bytes read via ATS and Extended\n",
    "\tGPU Memory traffic by the Socket 0 GPU to the Socket 0 CPU memory.\n",
    "\tIn a system with two SoCs connected via a C2C bus, counts the bytes read via remote socket PCIe traffic to\n",
    "\tthe Socket 0 CPU memory.\n",
    "\n",
    "'67'\tSocket_0_C2C0/total_bytes_local\n",
    "\tIn a system with the GPU connected to the SoC via the C2C bus, counts the bytes read and written via ATS\n",
    "\tand Extended GPU Memory traffic by the Socket 0 GPU to the Socket 0 CPU memory.\n",
    "\tIn a system with two SoCs connected via a C2C bus, counts the bytes read and\n",
    "\trelaxed-ordered bytes written via remote socket PCIe traffic to the Socket 0 CPU memory.\n",
    "\n",
    "'69'\tSocket_0_C2C0/total_requests_local\n",
    "\tIn a system with the GPU connected to the SoC via the C2C bus, counts the read and write requests via\n",
    "\tATS and Extended GPU Memory traffic by the Socket 0 GPU to the Socket 0 CPU memory.\n",
    "\tIn a system with two SoCs connected via a C2C bus, counts the read requests and relaxed-ordered write\n",
    "\trequests via remote socket PCIe traffic to the Socket 0 CPU memory.\n",
    "\n",
    "'71'\tSocket_0_C2C0/wr_bytes_local\n",
    "\tIn a system with the GPU connected to the SoC via the C2C bus, counts the bytes written via ATS and Extended\n",
    "\tGPU Memory traffic by the Socket 0 GPU to the Socket 0 CPU memory.\n",
    "\tIn a system with two SoCs connected via a C2C bus, counts the relaxed-ordered bytes written via remote socket\n",
    "\tPCIe traffic to the Socket 0 CPU memory.\n",
    "```\n",
    "\n",
    "Below command shows the list of available CPU core events that can be and the maximum number of CPU events that can be sampled concurrently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55985822-da22-4581-8e9a-f011c1ef54f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nsys profile --cpu-core-events=help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52eb5fe8-4def-4b8e-b8f4-8bbed55f2ce6",
   "metadata": {},
   "source": [
    "Now, let's profile the code using NVIDIA Nsight Systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fd6fc5-a6e3-4f82-b636-69989258f33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../source_code && nsys profile --flush-on-cudaprofilerstop=false --gpu-metrics-device=0 --gpu-metrics-frequency=20000 --trace=cuda,nvtx --event-sample=system-wide --cpu-socket-events='61,67,69,71' -o jacobi_report --force-overwrite true ./jacobi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963aa2c3-6063-47a6-bdc4-1c38658e2d8b",
   "metadata": {},
   "source": [
    "Let's checkout the profiler's report.  Download and save the report file by holding down <mark>Shift</mark> and <mark>right-clicking</mark> the [report](../source_code/jacobi_report.nsys-rep)  then choosing <mark>save Link As</mark>. Once done, open it via the GUI. Below is an example screenshot on the Grace Hopper superchip. It shows the collected uncore events in their own row (see red box in the screenshot). It counts the bytes read, written, the total bytes read and written as well as the counts of read and write requests via ATS. Hovering the cursor over an event sampling row in the timeline shows the event’s rate at that moment.\n",
    "\n",
    "<img src=\"../../_common/images/jacobi_uncore.png\" width=\"70%\" height=\"70%\">\n",
    "\n",
    "The second screenshot shows the CUDA API row and the kernel. CUDA API shows traces of CUDA Runtime calls made by the application (e.g: cudaMalloc).\n",
    "\n",
    "<img src=\"../../_common/images/jacobi_cuda_api.png\" width=\"70%\" height=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96c5d69-266e-4f69-b60e-59622972b8c2",
   "metadata": {},
   "source": [
    "## Lab Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9c70aa-1a01-4214-9796-45fd4d9b0e19",
   "metadata": {},
   "source": [
    "The code requires explicit memory copy API like **cudaMemcpy**.  The task in this section is to modify the source code [C/C++ version](../source_code/jacobi.cu) and remove any explicit memory allocation and memory copy API like **cudaMalloc** and **cudaMemcpy** . Replace the same with malloc and pass the same pointer to both CPU and GPU function calls. \n",
    "\n",
    "Compile and profile the code to analyze the timeline provided by the profiler. Note the absence of explicit calls to memory copy APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaea982-7116-46bf-8ddf-31715a4b3caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../source_code && make clean && make\n",
    "!cd ../source_code && nsys profile --flush-on-cudaprofilerstop=false --gpu-metrics-device=0 --gpu-metrics-frequency=20000 --trace=cuda,nvtx --event-sample=system-wide --cpu-socket-events='61,67,69,71' -o jacobi_report --force-overwrite true ./jacobi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a44361-28db-4578-947f-521c33b285a7",
   "metadata": {},
   "source": [
    "\n",
    "Feel free to checkout the solutions available at [C/C++ version](../source_code/jacobi_solution.cu) to help you understand better. Download and save the report file by holding down <mark>Shift</mark> and <mark>right-clicking</mark> the [report](../source_code/jacobi_report.nsys-rep)  then choosing <mark>save Link As</mark>. Once done, open it via the GUI. \n",
    "\n",
    "Below is an example screenshot on the Grace Hopper superchip. It shows the collected uncore events in their own row (see red box in the screenshot). When we replace the `cudaMalloc` calls with `malloc`, we can see more C2C activities comparing to when we use `cudaMalloc` to allocate memory on device. \n",
    "\n",
    "<img src=\"../../_common/images/jacobi_solution_uncore.png\" width=\"70%\" height=\"70%\">\n",
    "\n",
    "The below example screenshot shows that we have lots of `cudaMemcpy` as the data that was allocated using `malloc` is accessed directly by the GPU via using Automatic Translation Service. \n",
    "\n",
    "<img src=\"../../_common/images/jacobi_solution_cuda_api.png\" width=\"70%\" height=\"70%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a4d252-2257-4101-9980-292c13893c8e",
   "metadata": {},
   "source": [
    "## API Change and Coding Guidelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3659f0da-c81d-49dd-913c-486bfcffa116",
   "metadata": {},
   "source": [
    "Memory coherent architecture like Grace Hopper Superchip strives to make sure there is:\n",
    "- No programming model changes\n",
    "- No new APIs to be learned\n",
    "- No changes to existing APIs\n",
    "\n",
    "For example, the Unified Memory Programming feature used in the previous lectures of OpenACC, CUDA, and Standard Languages is supported for all platforms. Which primarily means memory accesses just work. \n",
    "\n",
    "Additionally,  hints can be provided and only impact performance, not results.\n",
    "- [cudaMemAdvise](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1ge37112fc1ac88d0f6bab7a945e48760a): PreferredLocation, AccessedBy.\n",
    "- [cudaMemPrefetch](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1ge8dc9199943d421bc8bc7f473df12e42): prefetch to NUMA node.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "753e9a65-25c0-4f2c-931c-c27d6da6dcf5",
   "metadata": {},
   "source": [
    "## CPU only\n",
    "\n",
    "OpenACC, Standard Languages  and OpenMP are supposed to work out of the box.\n",
    "\n",
    "### Compilers Support for Grace:\n",
    "**NVHPC**: \n",
    "- Focused on application performance,\n",
    "- High velocity, constant innovation\n",
    "\n",
    "**LLVM and Clang:**\n",
    "- NVIDIA provides builds of [Clang](https://developer.nvidia.com/grace/clang) for Grace\n",
    "- Drop-in replacement for mainline Clang\n",
    "- 100% of Clang enhancements for Grace are contributed to mainline LLVM\n",
    "\n",
    "**GCC**\n",
    "- NVIDIA contributes to mainline GCC to support Grace\n",
    "- Working with all major Linux distros to improve the availability of Grace optimizations in GCC\n",
    "\n",
    "#### Tips for porting from x86 to ARM CPU like \n",
    "- Remove all architecture-specific flags: `-mavx`,` -mavx2`, etc.\n",
    "- Remove `-march` and `-mtune` flags. \n",
    "- Use `-Ofast -mcpu=native`\n",
    "- If fast math optimizations are not acceptable, use `-O3 –ffp-contract=fast`\n",
    "- Use `–flto` to enable link-time optimization \n",
    "- Apps may need `-fsigned-char` or `-funsigned-char` depending on the developer’s assumption\n",
    "- `gfortran` may benefit from `-fno-stack-arrays`\n",
    "\n",
    "[Nvidia Performance Libraries](www.developer.nvidia.com/nvpl) allows easy porting of applications by providing drop-in replacement for any math library implementing standard interfaces (e.g. Netlib, FFTW).\n",
    "\n",
    "```\n",
    "gcc -DUSE_CBLAS -ffast-math -mcpu=native -O3 -I/PATH/TO/nvpl/include -L/PATH/TO/nvpl/lib -o mt-dgemm.nvpl mt-dgemm.c -lnvpl_blas_lp64_gomp\n",
    "\n",
    "```\n",
    "Other options include using libraries and frameworks like ATLAS, OpenBLAS, BLIS, which are community-supported with some optimizations. They work for Grace, but are unlikely to outperform NVPL. \n",
    "\n",
    "\n",
    "### SIMD \n",
    "\n",
    "ARM assembly is simpler than x86. Arm processors have a much simpler and general set of registers than x86. Just assign a one-to-one mapping from an x86 register to an Arm register when porting code. Complex x86 instructions will become multiple Arm instructions.\n",
    "\n",
    "Follow Arm’s documentation on rewriting x86 vector intrinsic:\n",
    "- SVE porting: https://developer.arm.com/documentation/101726/latest\n",
    "- NEON Porting: https://developer.arm.com/documentation/101725/0300/Coding-for-Neon \n",
    "\n",
    "\n",
    "# Links and Resources\n",
    "[NVIDIA Hopper GPU](https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/) \n",
    "\n",
    "[NVIDIA Grace CPU](https://www.nvidia.com/en-us/data-center/grace-cpu/)\n",
    "\n",
    "[NVIDIA Nsight System](https://docs.nvidia.com/nsight-systems/)\n",
    "\n",
    "\n",
    "**NOTE**: To be able to see the Nsight Systems profiler output, please download the latest version of Nsight Systems from [here](https://developer.nvidia.com/nsight-systems).\n",
    "\n",
    "Don't forget to check out additional [Open Hackathons Resources](https://www.openhackathons.org/s/technical-resources) and join our [OpenACC and Hackathons Slack Channel](https://www.openacc.org/community#slack) to share your experience and get more help from the community.\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429c6c5b-41b1-4adb-b125-f7a0e53399f1",
   "metadata": {},
   "source": [
    "## Licensing \n",
    "\n",
    "Copyright © 2022 OpenACC-Standard.org.  This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). These materials may include references to hardware and software developed by other entities; all applicable licensing and copyrights apply."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
